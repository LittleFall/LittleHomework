# 文件分割法

将大文件切割成若干个小文件，相同的url放入相同的小文件中，再分别每个小文件每个种类的出现次数，找到最大，然后合并。

核心在于如何将大文件分割成多个小文件。

### 可能出现的问题

如果某个小文件的url种类数（不是数量，而是种类数）非常多，导致内存无法容纳统计次数的哈希表，就会出现问题。

效率问题。

### 目标

分割尽量均匀，处理类数最多的小文件时占用的内存不能超过1GB.

效率要足够。

### 当前分割方法——哈希分割

预先确定小文件数，然后按url的hash值把每个url分入对应的小文件中。

##### 哈希函数1.

将字符串视为radix进制下的数字，然后对blocks取模。当前radix=33，blocks=13331.

评价：基本可以做到随机分割，效率适中。

##### 哈希函数2. times33改

将字符串视为33进制下的数字，使用unsigned保存计算过程，只在最后结束后取模。

评价：
1. 少了很多次取模运算，效率快；
2. 测试后的分割结果不如上一种方法平均，因为中间过程相当于对2^64次取模，因为不是质数所以不够均匀。

##### 哈希函数3. crc32+移位
参考：[memcached](http://lists.danga.com/pipermail/memcached/2004-June/000664.html)

